{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow & Keras\n",
    "\n",
    "## 학습목표\n",
    "\n",
    "- 신경망 구현을 위한 패키지\n",
    "- Tensorflow 란?\n",
    "- Tensorflow 환경 구축\n",
    "- Keras 란?\n",
    "- Keras 환경 구축\n",
    "- Keras 를 이용한 MNIST 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망 구현을 위한 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리는 인공신경망이 어떤 원리로 동작하는지에 대해 살펴보았고, 각 단계마다 직접 그것을 구현했습니다. 지난 강의에서는 MNIST 데이터셋에 적용을 해보았습니다. 혹시 여러분이 코딩하는데 어려움이 있었을지도 모르겠습니다. 인공 신경망의 은닉층이 많아지거나, 데이터의 특성에 따라 신경망에 새로운 구조를 추가시켜야 할 경우도 있습니다. 이러할 때마다 우리가 앞서 구현했던 NeuralNetwork 클래스에 새로운 기능을 추가하기에는 많은 시간과 노력이 따를 것입니다.\n",
    "\n",
    "다행히도, 우리는 신경망을 사용할때마다 이와같이 부가적인 코딩을 하지 않아도 됩니다. 이러한 목적으로 개발된 패키지가 많이 있으며, 그 중 유명한 몇가지를 소개한다면 다음과 같습니다:\n",
    "\n",
    "- [Tensorflow](https://www.tensorflow.org/)\n",
    "- [Keras](https://keras.io/)\n",
    "- [Pytorch](https://pytorch.org/)\n",
    "- [Caffe](http://caffe.berkeleyvision.org/)\n",
    "- [Theano](http://deeplearning.net/software/theano/)\n",
    "- [Scikit-learn](http://scikit-learn.org/)\n",
    "\n",
    "이번 강의에서는 Tensorflow 를 소개하겠습니다. Tensorflow 를 사용한다면 간단하게 신경망을 구성할 수 있습니다. 데이터를 읽어드린 다음, 신경망을 구성하고 학습시켜 결과를 예측하는 것을 보여드리겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensorflow 란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tensorflow.PNG\" width=\"500\">\n",
    "<center>그림 1: Tensorflow [출처](https://tensorflow.org/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow는 구글이 오픈 소스로 개발하여, 이 분야의 주류가 되어가고 있습니다. 핵심 기술은 C++로 작성되었지만,  프론트 엔드 부분은 파이썬으로 작성되어서, 파이썬으로 쉽게 접근이 가능합니다. \n",
    "Tensorflow를 이용해서 이미지, 음성, 비디오 등 다양하고 많은 자료를 기계학습 할 수 있으며, GPU(Graphics Processing Unit)와 같은 하드웨어를 사용합니다. <br>\n",
    "\n",
    "참고로, 수학에서 0차원의 수를 스칼라$^{scalar}$, 1차원 배열을 벡터$^{vector}$, 2차원 배열을 행렬$^{matrix}$라고 부릅니다.  이와 같이 스칼라, 벡터와 행렬을 일반화한 것을 텐서$^{tensor}$라고 합니다. \n",
    "TensorBoard 라는 훌륭한 시각화 툴을 제공하고 있어 학습 과정을 모니터링 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensorflow 환경 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 를 사용하기 위해서는 Python 3.6 이상 버전이 설치되어야 합니다. 하지만, 우리는 Python이 포함된 아나콘다를 이용해서 Tensorflow를 설치할 것이기 때문에, Python을 따로 설치하지는 않아도 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Anaconda 설치\n",
    "\n",
    "Anaconda3 를 설치하세요. 다음 링크를 통해 설치하시면 됩니다: [링크](https://www.continuum.io/downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensorflow 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OS X or Linux\n",
    "\n",
    "아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.6\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "#### Windows\n",
    "\n",
    "cmd 혹은 Anaconda 쉘에서 아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.6\n",
    "activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "Tensorflow 가 정상적으로 설치되었는지 확인하기 위해 아래의 코드를 실행해보세요. tensorflow 라이브러리를 import 할 때에 문제가 생기지 않는다면, 정상적으로 설치된 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설치중에 어려움이 있다면, 아래 링크를 참고하도록 합니다.\n",
    "\n",
    "- [Anaconda 설치](https://www.continuum.io/downloads)\n",
    "- [tensorflow 설치](https://www.tensorflow.org/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras 란?\n",
    "<img src=\"images/keras.PNG\" width=\"500\">\n",
    "<center>그림 1: Keras [출처](https://keras.io/)</center>\n",
    "\n",
    "Keras 는 Python 기반으로 쓰여졌으며 TensorFlow 혹은 CNTK, Theano 위에서 실행되는 신경망 API입니다. 문법을 간단하게 그리고 직관적으로 만들어 쉽게 기계학습을 구현할 수 있도록 만들어졌습니다. Keras 를 사용해서 CNN(Convolutional Neural Network)과 RNN (Recurrent Neural Network)도 구현할 수 있으며 두가지 딥러닝 모델을 섞어서 구현하는 것도 가능합니다. 또한 하나의 코드로 CPU와 GPU를 사용하는 기계에서 모두 동작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keras 환경 구축\n",
    "\n",
    "앞에서 Tensorflow를 설치했다면, Keras를 사용할 수 있습니다. 다음과 같이 keras를 사용할 수 있습니다.\n",
    "\n",
    "```python\n",
    "import tensorflow.keras as keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Keras 를 이용한 MNIST 데이터 분석\n",
    "이제 Keras를 사용할 환경이 구축되었습니다. Keras를 이용해서 MNIST 데이터를 분석해보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 MNIST 데이터 읽어오기\n",
    "\n",
    "Keras 에는 datasets 라는 모듈이 있으며, 사람들이 많이 사용하는 데이터를 쉽게 사용할 수 있도록 만들어져 있습니다. MNIST 데이터를 읽어오기 위해서 아래와 같이 두 줄이면 충분합니다. keras.datasets 에 있는 mnist 클래스에서 load_data 메소드를 호출하면 MNIST 데이터를 임의로 섞어서 학습시킬 때 사용할 데이터와 테스트할 때 사용할 데이터로 나눠줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나뉘어진 데이터 세트의 형상은 아래와 같습니다. 학습할 데이터에는 60,000개의 입력값이, 테스트할 `X` 데이터에는 10,000개의 입력값이 저장되어있는 것을 볼 수 있습니다. 각각의 입력값은 28 x 28 의 크기로 0 ~ 255 범위에 있는 숫자가 저장됩니다. `y` 에는 해당 입력값의 실제 숫자 0 ~ 9가 저장됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (60000, 28, 28)\n",
      "y_train.shape: (60000,)\n",
      "X_test.shape: (10000, 28, 28)\n",
      "y_test.shape:(10000,)\n",
      "\n",
      "X_train: minimum value=0, maximum value=255\n",
      "X_test: minimum value=0, maximum value=255\n",
      "y_train: minimum value=0, maximum value=9\n",
      "y_test: minimum value=0, maximum value=9\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: {}\\ny_train.shape: {}\\nX_test.shape: {}\\ny_test.shape:{}\\n\".\n",
    "      format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "print(\"X_train: minimum value={}, maximum value={}\".format(X_train.min(), X_train.max()))\n",
    "print(\"X_test: minimum value={}, maximum value={}\".format(X_test.min(), X_test.max()))\n",
    "print(\"y_train: minimum value={}, maximum value={}\".format(y_train.min(), y_train.max()))\n",
    "print(\"y_test: minimum value={}, maximum value={}\".format(y_test.min(), y_test.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 모든 feature의 값을 0 ~ 1 사이로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습을 하게되면 일반적으로 모든 feature를 0 에서 1 사이의 값으로 맞추어서 모델을 학습하곤 합니다. 여러 이유가 있겠지만, (1) 각 입력값의 특정 feature 가 큰 범위로 변할 경우 다른 feature의 영향력이 모델링 하는 단계에서 무시될 수 있기도 하며 (2) 비용함수를 계산하는 과정에서 값이 너무 크게되는 경우도 발생합니다.\n",
    "\n",
    "MNIST 데이터는 각 feature들이 왼쪽 위에서부터 오른쪽 아래까지의 pixel 값들이기 때문에, 모든 feature들의 최소값은 0이고 최대값은 255 입니다. 따라서 모든 feature들의 값을 255로 나눠주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 정수로 저장된 `y` 값을 one-hot 으로 바꿔주겠습니다. `keras.utils` 에 있는 `np_utils` 메소드를 사용해서 쉽게 해결할 수 있습니다. 아래의 간단한 예제는 정수로 저장되어있는 `one_to_ten` 넘파이 어레이를 one-hot 으로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import utils\n",
    "import numpy as np\n",
    "\n",
    "one_to_ten = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "one_to_ten = tf.keras.utils.to_categorical(one_to_ten, 10)\n",
    "print(one_to_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 one-hot 인코딩\n",
    "이제 `y_train` 과 `y_test` 를 one-hot 인코딩 하겠습니다. `y_train` 의 처음 5개 레이블을 `[5 0 4 1 9]` 입니다. one-hot 인코딩이 정말 간단하죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous five labels in y_train: [5 0 4 1 9]\n",
      "One-hot encoded labels of y_train: \n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"previous five labels in y_train: {y_train[:5]}\")\n",
    "y_train = utils.to_categorical(y_train, 10)\n",
    "y_test = utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"One-hot encoded labels of y_train: \\n{y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 신경망 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 신경망을 구축합니다. 가장 간단한 종류의 신경망힌 `Sequential` 을 구축하겠습니다. `Sequential` 은 우리가 지금까지 배운 내용과 같이 단계적으로 층을 쌓아가는 것입니다. `Sequential()` 로 모델을 정의해주고, 층을 더할 때는 `add` 메소드를 사용하면 됩니다. 처음 입력값의 feature들이 몇개가 되는지 `model.add(Flatten(input_shape=X_train.shape[1:]))` 를 통해 알려주며, `Dense` 층 즉, 완전이 연결된 신경망을 더해줍니다. 1개의 은닉층에 총 512개의 뉴런을 사용하고 활성화함수로트 `relu` 를 사용하겠습니다.\n",
    "\n",
    "Dropout(드롭아웃)은 앞에서 공부한대로, 은닉층에서 발생하는 과적합(overfitting)을 방지하기 위하여, 임의로 20~50% 정도의 노드(뉴론)들을 학습 과정에서 제외시키는 것입니다. 여기서는 20%로 설정합니다. \n",
    "\n",
    "마지막 층은 10개의 뉴런을 사용하며, 각각의 뉴런은 0부터 9까지의 숫자를 예측하는 역할을 합니다. 활성화함수로 `softmax` 를 사용하여서, 특정 입력값이 들어왔을 때, 마지막 층의 뉴런이 출력해내는 값의 합이 1이 되게끔 합니다. 즉, 해당 입력값에 대해 그것이 0일 확률 부터 9일 확률까지를 보여주는 것으로 해석할 수도 있습니다.\n",
    "\n",
    "`model.summary()` 를 해주면, 우리가 구축한 모델을 요약해서 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))    # input_shape = (28, 28)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 컴파일\n",
    "자, 이제 신경망을 구축했으니 모델을 컴파일하면 되겠군요. 이 단계에서는 어떤 비용함수를 쓸 것인지 (loss), 어떠한 최적화기를 사용할 것인지 (optimizer), 무엇을 기준으로 학습 할 것인지 (metrics) 를 설정해줍니다. 아래의 조합은 분류를 할 때에 일반적으로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 모델 학습\n",
    "이제 모델을 학습시키는 단계입니다. `keras.callbacks` 에 있는 `ModelCheckpoint` 메소드를 사용하면 학습 결과가 좋아질 때 마다 그때의 가중치를 지정한 `filepath` 에 저장할 수 있습니다.\n",
    "\n",
    "`fit` 메소드를 사용하면 학습을 시작합니다. training 데이터를 학습할 때 사용 할 데이터와 검증 집합 (validation set) 으로 나누어서 각 `epoch` 마다 검증 집합의 loss 가 증가한다면 학습이 되고있다고 볼 수 있습니다. 그리고 각 `epoch` 마다 한번에 몇 묶음의 입력값을 사용하여 동시에 가중치를 변화시킬 것인지에 해당하는 `batch_size` 를 지정할 수 있습니다.\n",
    "\n",
    "모델을 학습하는 중에 가중치를 계속 갱신하면서 가장 좋은 가중치를 지정한 파일(여기서는 `minist_best.h5`)에 저장합니다. 다른 말로, 이 학습 과정 중에 성능이 향상될 때마다 지정된 파일로 가중치가 저장된다는 것입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.9126\n",
      "Epoch 00001: val_loss improved from inf to 0.15388, saving model to mnist_best.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3045 - accuracy: 0.9126 - val_loss: 0.1539 - val_accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9610\n",
      "Epoch 00002: val_loss improved from 0.15388 to 0.10418, saving model to mnist_best.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1313 - accuracy: 0.9609 - val_loss: 0.1042 - val_accuracy: 0.9688\n",
      "Epoch 3/10\n",
      "363/375 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9724\n",
      "Epoch 00003: val_loss improved from 0.10418 to 0.08859, saving model to mnist_best.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0919 - accuracy: 0.9724 - val_loss: 0.0886 - val_accuracy: 0.9740\n",
      "Epoch 4/10\n",
      "366/375 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9798\n",
      "Epoch 00004: val_loss improved from 0.08859 to 0.08583, saving model to mnist_best.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0681 - accuracy: 0.9799 - val_loss: 0.0858 - val_accuracy: 0.9754\n",
      "Epoch 5/10\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9827\n",
      "Epoch 00005: val_loss improved from 0.08583 to 0.07550, saving model to mnist_best.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0554 - accuracy: 0.9827 - val_loss: 0.0755 - val_accuracy: 0.9794\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9861\n",
      "Epoch 00006: val_loss did not improve from 0.07550\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 0.0885 - val_accuracy: 0.9760\n",
      "Epoch 7/10\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0377 - accuracy: 0.9883\n",
      "Epoch 00007: val_loss did not improve from 0.07550\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0376 - accuracy: 0.9883 - val_loss: 0.0762 - val_accuracy: 0.9790\n",
      "Epoch 8/10\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9904\n",
      "Epoch 00008: val_loss did not improve from 0.07550\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0311 - accuracy: 0.9904 - val_loss: 0.0806 - val_accuracy: 0.9788\n",
      "Epoch 9/10\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0270 - accuracy: 0.9914\n",
      "Epoch 00009: val_loss did not improve from 0.07550\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0271 - accuracy: 0.9914 - val_loss: 0.0794 - val_accuracy: 0.9786\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9926\n",
      "Epoch 00010: val_loss did not improve from 0.07550\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0831 - val_accuracy: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x231ce38f310>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist_best.h5', \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[checkpointer],\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 분류 정확도 측정\n",
    "검증 집합에서 최고의 성능을 보여주는 모델의 가중치가 `mnist.model.best.hdf5` 에 저장되어 있습니다. 이 가중치를 불러온 후에, 테스트 세트를 예측해봅니다.\n",
    "\n",
    "`evaluate` 메소드를 사용해서, 테스트 세트에 있는 입력값들의 레이블을 예측합니다. `evaluate` 메소드는 두 가지 인자를 반환하는데, `loss` 값과 `metric` 에 지정한 값입니다. `loss` 는 학습시킬 때 사용한 것이고, `metric` 은 우리가 앞서 지정한 정확도를 의미합니다. \n",
    "\n",
    "코드를 실행할 때마다 accuracy 는 차이가 있겠지만 위와 같은 인자들로 신경망을 학습시켰을 때에 98% 전후의 정확도를 보여줍니다. 즉, 0에서 9까지 적혀있는 28 x 28 사이즈의 입력값이 들어왔을 때에, 98% 정확도로 어떤 숫자인지 알아내는 신경망을 학습시킨 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.00999760627747%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('mnist_best.h5')\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test)\n",
    "accuracy = 100 * loss_and_metrics[1]\n",
    "\n",
    "print(\"Test accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Keras를 이용한 CNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN을 이용하여 MNIST를 학습하기 위해서는 일반적인 Dense 층을 사용하는 것이 아닌,\n",
    "Convolutional Layer를 나타내는 Conv2D를 이용해야 합니다. \n",
    "코드에서는 Convolutional Layer에서 2 x 2 필터 16개를 사용한다는 뜻입니다. 그 다음으로는 Convolutional Layer의 dimensionality를 줄여주기 위해 MaxPooling을 진행하는 코드입니다. 이제 이 CNN을 모델을 학습시키고, MNIST 데이터를 분류하도록 테스트해봅시다. 테스트까지의 과정은 이미 앞에서 설명드렸으므로, 간단하게 넘어가겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 27, 27, 16)        80        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 27, 27, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 32)        2080      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 64)          8256      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 12,986\n",
      "Trainable params: 12,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2,\n",
    "                 padding='valid', activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2,\n",
    "                 padding='valid', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2,\n",
    "                 padding='valid', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.5798 - accuracy: 0.8251\n",
      "Epoch 00001: val_loss improved from inf to 0.30389, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.5789 - accuracy: 0.8253 - val_loss: 0.3039 - val_accuracy: 0.9363\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9326\n",
      "Epoch 00002: val_loss improved from 0.30389 to 0.19060, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.2200 - accuracy: 0.9326 - val_loss: 0.1906 - val_accuracy: 0.9603\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9482\n",
      "Epoch 00003: val_loss improved from 0.19060 to 0.14714, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 12s 33ms/step - loss: 0.1675 - accuracy: 0.9482 - val_loss: 0.1471 - val_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9563\n",
      "Epoch 00004: val_loss improved from 0.14714 to 0.12858, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.1387 - accuracy: 0.9563 - val_loss: 0.1286 - val_accuracy: 0.9735\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9629\n",
      "Epoch 00005: val_loss improved from 0.12858 to 0.11824, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 12s 33ms/step - loss: 0.1202 - accuracy: 0.9629 - val_loss: 0.1182 - val_accuracy: 0.9758\n",
      "Epoch 6/10\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9675\n",
      "Epoch 00006: val_loss improved from 0.11824 to 0.09492, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 13s 33ms/step - loss: 0.1031 - accuracy: 0.9675 - val_loss: 0.0949 - val_accuracy: 0.9789\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9703\n",
      "Epoch 00007: val_loss improved from 0.09492 to 0.08732, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 13s 33ms/step - loss: 0.0949 - accuracy: 0.9703 - val_loss: 0.0873 - val_accuracy: 0.9811\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9714\n",
      "Epoch 00008: val_loss improved from 0.08732 to 0.08293, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0884 - accuracy: 0.9714 - val_loss: 0.0829 - val_accuracy: 0.9809\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9738\n",
      "Epoch 00009: val_loss improved from 0.08293 to 0.07898, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 15s 39ms/step - loss: 0.0829 - accuracy: 0.9738 - val_loss: 0.0790 - val_accuracy: 0.9843\n",
      "Epoch 10/10\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9757 E\n",
      "Epoch 00010: val_loss improved from 0.07898 to 0.07651, saving model to mnist.model.best.hdf5\n",
      "375/375 [==============================] - 16s 44ms/step - loss: 0.0781 - accuracy: 0.9758 - val_loss: 0.0765 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x231cec599d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "                filepath='mnist_best_cnn.h5',\n",
    "                verbose=1,\n",
    "                save_best_only=True)\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128, epochs=10,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[checkpointer],\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 결과 98.54%의 정확도가 나오는 것을 볼 수 있습니다. 기존 신경망보다 미묘하지만 조금 더 정확도가 커진 것을 볼 수 있지요.테스트 데이터가 10,000장인 것을 생각했을 때, 0.54%가 증가한 것은 54개의 이미지를 더 정확히 분류했다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1174 test_step\n        y_pred = self(x, training=False)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 28, 28]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12216/1257880832.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist.model.best.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss_and_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1174 test_step\n        y_pred = self(x, training=False)\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('mnist_best_cnn.h5')\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test)\n",
    "accuracy = 100 * loss_and_metrics[1]\n",
    "\n",
    "print(\"Test accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Tensorflow 공식 홈페이지](https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners)\n",
    "- Udacity 강의 'Artificial Intelligence Nanodegree - Convolutional Neural Networks' [aind2-cnn Jupyter Notebook 파일](https://github.com/udacity/aind2-cnn/blob/master/mnist-mlp/mnist_mlp.ipynb)\n",
    "- Keras Documentation https://keras.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
